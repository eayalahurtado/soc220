{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "# Soc220: Computational Text Analysis\n",
    "## Lab2: Webscraping With Beautiful Soup\n",
    "\n",
    "<br>\n",
    "\n",
    "![pip](https://www.pcs.org/assets/uploads/GE_Illustration_Large750.jpg)\n",
    "\n",
    "***\n",
    "    2/1/2018\n",
    "    (Image: Pip and Magwitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primes = []\n",
    "for j in range(2,101):\n",
    "    #count up to each number\n",
    "    for i in range(2,j):\n",
    "        #check if there are any numbers that can divide\n",
    "        if j % i == 0:\n",
    "            #and if there are, stop\n",
    "            break\n",
    "    else:\n",
    "        #since not, print\n",
    "        primes.append(j)\n",
    "        print(j)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isprime(j):\n",
    "    '''\n",
    "    Takes in a positive number and checks whether it is prime. Returns number if prime.\n",
    "    \n",
    "    Int -> Int\n",
    "    '''\n",
    "    for i in range(2,j):\n",
    "        if j % i == 0:\n",
    "            break     \n",
    "    else:\n",
    "        return(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isprime(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isprime(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isprime(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "primes_to_100 = [k for k in range(1, 100) if isprime(k)]\n",
    "primes_to_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "Today we're going to scrape some data from the court records from the Old Bailey, the criminal court of London from the mid-17th century up until WW1. Thanks to the British National Library, every single criminal proceeding (transcripts, charges, verdict sentences etc) are freely accesible online. This week, we'll scrape from the front-end of the website, that is, we'll write some code to automatically load a webpage, find some content, and then save it. \n",
    "\n",
    "A few notes on webscraping:\n",
    "- This is an exremely flexible way to collect data. If you can view it in a browser, then it's possible, with enough patience, to get that data.\n",
    "- However, the way we teach a computer to get these data is to look through the HTML code for specific tags, which means that if the web-developer for a given website decides to make a slight change in that coding or if they themselves screwed-up or if the code is a bit inconsistent, then you can be SOL in getting data. Possibly creating bias in your collection process!\n",
    "- In addition, we'll probably have to use 'regular expressions' which is a very old-school coding language all unto itself for parsing strings. We'll use this to sort through the types of text that we want which are held within specific HTML tags.\n",
    "\n",
    "The new [firefox browser](https://www.mozilla.org/en-US/firefox/) is great for viewing html code on a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, we load libraries: \n",
    "######\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "![obo_online](obo_online.png)\n",
    "\n",
    "\n",
    "***\n",
    "<font color=darkgreen>\n",
    "    \n",
    "Charles Dickens was once a court reporter at the Old Bailey and witnessing this case was the inspiration for the character of Magwitch, the plot twist of Great Expectations.\n",
    "\n",
    "\n",
    "[Persecution of Thomas Knight](https://www.oldbaileyonline.org/browse.jsp?id=t18331017-6-off35&div=t18331017-6)\n",
    "\n",
    "![knight](knight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "## General strategy:\n",
    "\n",
    "1. Find a list of links, each of which contains some text that we are looking for. In other contexts, this could be a list of speeches, a list of press releases, a list of publications, a list of novels even -- basically anything.\n",
    "2. Identify a regular expression or HTML code tag that on each of those pages gives us the text in question that we want.\n",
    "3. Extract that text, pause for a moment (n.b. ALWAYS have a built-in pause in order to not alarm website admins. For those of you who decide to become black-hat hackers, this automated-request of a website is identical to what a DDOS is), then move on to the next page and rinse and repeat.\n",
    "4. We'll also want to include error code expections so that if one page has some issue with it (null value for instance), we don't want the entire scrape to end, especially if we are planning to scrape over the course of 10 to 12 hours. We will wish to record that fact as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "### Step 1: Figuring out the text data we want.\n",
    "\n",
    "Here I wish to see all cases which involved Britons who were exiled (and then came back).\n",
    "\n",
    "\n",
    "\n",
    "Next week, we'll use the full API. And we'll also discuss going to 'private' web pages.\n",
    "\n",
    "https://www.oldbaileyonline.org/forms/formCustom.jsp\n",
    "\n",
    "'The boxes below allow you to search the whole of the Proceedings and all published Ordinary of Newgate's Accounts (for the period from 1679 to 1772)'\n",
    "\n",
    "Search for those tried for 'returning from transportation' (same sentence as Magwitch):\n",
    "\n",
    "https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&start=0&count=0\n",
    "\n",
    "So, from this angle, we'll get a piece of what we want, but next week, with APIs, we'll dig a little deeper.\n",
    "\n",
    "Let's start with this case, someone convicted of stealing a handkerchief:\n",
    "\n",
    "https://www.oldbaileyonline.org/browse.jsp?id=t17940115-36-off187&div=t17940115-36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first take the URL \n",
    "url = \"https://www.oldbaileyonline.org/browse.jsp?id=t17940115-36-off187&div=t17940115-36\"\n",
    "# and then we request it, as if we are loading an indiviudal web page.\n",
    "req = requests.get(url,timeout=20) #always include a delay!\n",
    "#For those curious,the actual code for a DDoS attack is pretty similar to page request.\n",
    "req.status_code #200 means we have gotten it correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "req.html = req.text #extract the text from the request\n",
    "soup = BeautifulSoup(req.html,\"html.parser\") #parse the text so the computer can read the html tags\n",
    "print(soup.prettify()[2500:3000]) #print out what the soup looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![html_code_structure](http://www.openbookproject.net/tutorials/getdown/css/images/lesson4/HTMLDOMTree.png)\n",
    "\n",
    "<font color=darkgreen>\n",
    "    \n",
    "**Essentially what we're doing is looking for that last row down here, in order to find the relevant text.**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![html_dom](http://www.cs.toronto.edu/~shiva/cscb07/img/dom/treeStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### **Html code structure**\n",
    "\n",
    "- Upon request, HTML is parsed into DOM (DOcument Object Model)\n",
    "- All content we want is generally at bottom of this tree.\n",
    "- The 'art' / frustrating part of webscrapping is identifying what piece of the treet to navigate down.\n",
    "- The breakpoints for a webscrape are when this tree has some inconsistency (null data or things are structured differently.\n",
    "- So again, if you can view it in a browser, then you can scrape it. If there's any break in this structure, then your code is going to fail b/c it relies on this structure to navigate for data.\n",
    "\n",
    "\n",
    "### Navigating this structure:\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree\n",
    "\n",
    "1. Find a given tag\n",
    "2. Find a tag and ask for its children\n",
    "3. Find a tag and ask for parents\n",
    "4. Ask for siblings (navigate sideways)\n",
    "\n",
    "Otherwise:\n",
    "\n",
    "1. Search for an expression `soup.find_all('b')`\n",
    "2. Search using a regular expression. `re.compile(\"t\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### Regular expressions\n",
    "\n",
    "- Functionally, another distinct language\n",
    "- Raw method of parsing strings\n",
    "\n",
    "Regular expression cheat sheet:\n",
    "\n",
    "https://regexr.com\n",
    "\n",
    "Datacamp tutorial:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/python-regular-expression-tutorial\n",
    "\n",
    "Python docs:\n",
    "\n",
    "https://docs.python.org/2/library/re.html\n",
    "\n",
    "Table:\n",
    "\n",
    "| Expression | Search                         |\n",
    "|------------|--------------------------------|\n",
    "| \\d         | Any Digit                      |\n",
    "| \\D         | Any Non-digit character        |\n",
    "| .          | Any Character                  |\n",
    "| \\.         | Period                         |\n",
    "| [abc]      | Only a, b, or c                |\n",
    "| [^abc]     | Not a, b, nor c                |\n",
    "| [a-z]      | Characters a to z              |\n",
    "| [0-9]      | Numbers 0 to 9                 |\n",
    "| \\w         | Any Alphanumeric character     |\n",
    "| \\W         | Any Non-alphanumeric character |\n",
    "| {m}        | m Repetitions                  |\n",
    "| {m,n}      | m to n Repetitions             |\n",
    "| *          | Zero or more repetitions       |\n",
    "| +          | One or more repetitions        |\n",
    "| ?          | \"zero or one occurrences of the preceding element.\"              |\n",
    "| \\s         | Any Whitespace                 |\n",
    "| \\S         | Any Non-whitespace character   |\n",
    "| ^…$        | Starts and ends                |\n",
    "| (a(bc))    | Capture Sub-group              |\n",
    "\n",
    "https://github.com/zeeshanu/learn-regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Let's imagine we have a series of webpages with this text on it, a bit of \n",
    "#the Captain Ahab's final words\n",
    "\n",
    "class_sentences = [\"Monday: Homework is due at 11:59am\",\n",
    "                  \"Wednesday: Class is at 2:00pm. Review papers!\",\n",
    "                  \"Thursday: Lab is at 5:00pm\"]\n",
    "\n",
    "df = pd.DataFrame(class_sentences, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select dataframe, the column 'final_words', call str, and then use, len\n",
    "#method on that string\n",
    "# find number of characters\n",
    "df['text'].str.len()\n",
    "\n",
    "# n.b. if you wish to convert something to a string, wrap it in a str() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "Are there 177 words or 177 characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens for each string \n",
    "# call the split function to break up on white spaces, and then count len of \n",
    "#those split up strings\n",
    "df['text'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search through final words, use string method of contains to find 'Whale'\n",
    "df['text'].str.contains('Lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regualar expressions being w/ a r before the string construct quotes\n",
    "# \\d looks for digits\n",
    "df['text'].str.contains(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group and find digits\n",
    "df['text'].str.findall(r'(\\d?\\d):(\\d?\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all 'I'\n",
    "df['text'].str.extractall(r'(is)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#between two digits ':\" then zero or more of the group of either [ap] m\n",
    "df['text'].str.extractall(r'((\\d?\\d):(\\d\\d) ?([ap]m))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### Open developer tools. Tools --> Toggle Developer Tools\n",
    "\n",
    "https://www.oldbaileyonline.org/browse.jsp?id=t17940115-36-off187&div=t17940115-36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.html,\"html.parser\") #n.b. different parsers can mean very different structures\n",
    "print(soup.prettify()[9500:10500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 20 tags\n",
    "[tag.name for tag in soup.find_all()][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the head, is there anything useful up here?\n",
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's find all paragraphs of text in the body of the webpage.\n",
    "soup.body.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result of this search is a bs4 result\n",
    "type(soup.body.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we have to use the .text or .string method to extact the actual string or text that we want\n",
    "for p_tag in soup.find_all('p'):\n",
    "    print(p_tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "However, I get all this crap at the end that I don't want to have in there. Plus I also want to extract some unqiue values: name of target, date, their sentence, and then the text of the trial. This is is all in one big blob.\n",
    "\n",
    "Let's say I wanted to create this Python data structure (next time we'll discuss storing this externally):\n",
    "\n",
    "| Defendant Name | Date of Trial | Sentence | Text of Trial |\n",
    "|----------------|---------------|----------|---------------|\n",
    "| Joshua Daniels  | 1/15/1794      | Death    |  JOHN OWEN sworn. I am a servant to Mr. Kirby. The prisoner was tried here in September sessions last, for stealing an handkerchief, and was convicted, and received sentence to be transported....  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "### Step 2. Parse the html with BeautifulSoup, that is, turn the raw text into something that the machine can read through.\n",
    "\n",
    "BeautifulSoup has a few crucial functions for us:\n",
    "\n",
    "- `soup.prettify()`: Returns cleanedup version of raw html for printing\n",
    "- `soup.find_all()`: Returns Python list of matching objects\n",
    "- `soup.find()`:Returns first matching object for that item.\n",
    "- `soup.text/soup.get_text()`: Returns visible text of an object (e.g.,\"<p>Some text</p>\" -> \"Some text\")\n",
    "\n",
    "\n",
    "Full documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "#### Right-click, 'View source'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "So, let's go look at the developer tools on each page and see if we can find a structure that works. First the title:\n",
    "\n",
    "![](title.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first the title\n",
    "soup.find('div', class_='sessionsPaperTitle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just the text of the title\n",
    "soup.find('div', class_='sessionsPaperTitle').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "**Next, just the text of the trial.**\n",
    "\n",
    "And then next, the body, with the text in question:\n",
    "\n",
    "![](body.png)\n",
    "\n",
    "[//]: ![body_trail](body_detail.png)\n",
    "\n",
    "What we see here is that we want all the paragraphs within \"div sessions paper\" tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', class_='sessionsPaper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just the paragraphs in the div class=_sessionsPaper tag\n",
    "soup.find('div', class_='sessionsPaper').find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for p in soup.find('div', class_='sessionsPaper').find_all('p'):\n",
    "#    #get text per tag from within.\n",
    "#    print(p.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so, if we wish to write all these paragraphs into a single entity, we'll take them as a list.\n",
    "\n",
    "trial_text = [] #blank list, must be outside of 4 loop\n",
    "for p in soup.find('div', class_='sessionsPaper').find_all('p'):\n",
    "    trial_text.append(p.get_text()) #append each paragraph to blank list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check, list of paragraphs from website.\n",
    "trial_text[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "So, now we are going to take out a string with some identifying information in it and we're going to extract a list of paragraphs. At this point, you might be asking \"Zach, what the hell, data in this form is useless to me!\" Well, yes, we're going to start data wrangling next week. For right now, we just want to get it off the website and into a Python data object and save it locally.\n",
    "\n",
    "Later in your text analysis career, you'll find it useful to integrate cleaning and scraping simultaneously, right now, we're just going to focus on scrapping.\n",
    "\n",
    "** \n",
    "However, now we know what we want from each page: a single string which contains information about the trial (name of defendant and date of trial) and then a list of strings of the actual text of the case.)\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark>\n",
    "#### A word of warning: XML VS HTML\n",
    "\n",
    "- HTML tells your browser how to display information\n",
    "- XML is a rich-text document which lists which information is which.\n",
    "- One can save things directly as XML if one has the local storage space.\n",
    "- Instead of using an HTML parser, instead get the XML page and then parse that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view as XML:\n",
    "\n",
    "https://www.oldbaileyonline.org/browse.jsp?foo=bar&path=sessionsPapers/17940115.xml&div=t17940115-36&xml=yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<font color=darkgreen>\n",
    "    \n",
    "### 3. Get a list of links to the pages with the data.\n",
    "\n",
    "https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&start=0&count=0\n",
    "\n",
    "#### Here we encounter where the web design is inconsistent. We can't deal with 'start=0' so we have to 'hack it' and manually add that page at the end.\n",
    "\n",
    "https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&count=391&start=10\n",
    "\n",
    "https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&count=391&start=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so here is the base url from our search\n",
    "url_search = 'https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&count=391&start='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we also want to count from 0 to 390 in increments of 10\n",
    "range(10,400,10)\n",
    "print(list(range(10,390,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_urls = []\n",
    "for i in range(10,400,10):\n",
    "    #append to our blank list the base url append to the \n",
    "    list_urls.append(url_search+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hack and add this in at the end.\n",
    "\n",
    "list_urls.append('https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&start=0&count=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first URL, which is the second page.\n",
    "list_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the last URL, which is the first page.\n",
    "list_urls[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "So, now we have a list of URLS which contain a list of the pages of interest. Now we have to extract urls of the pages of interest.\n",
    "\n",
    "https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&start=0&count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = \"https://www.oldbaileyonline.org/search.jsp?gen=1&form=searchHomePage&_offences_offenceCategory_offenceSubcategory=miscellaneous_returnFromTransportation&start=0&count=0\"\n",
    "req = requests.get(url_list,timeout=20)\n",
    "req.status_code #again check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req.html = req.text\n",
    "soup = BeautifulSoup(req.html,\"html.parser\")\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# href=True specifies that there is a href attribute in an 'a' tag\n",
    "#soup.find_all('a', href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "Go to search results page and then get a link to the page trial page.\n",
    "\n",
    "![search_results](search_results.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use a regular expression on the href to select only those hrefs that start with the unique string\n",
    "# each tag option can be subsetted to select a part\n",
    "for a_tag in soup.find_all('a', href = re.compile('#highlight')):\n",
    "    print(a_tag['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "### **Now we extract the links to the unique cases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n.b. we gotta keep these outside for-loops\n",
    "\n",
    "list_trial_urls = []\n",
    "counter = 0\n",
    "\n",
    "for url_ in list_urls:\n",
    "    counter += 1\n",
    "    \n",
    "    #get url and soup the page\n",
    "    url = url_\n",
    "    req = requests.get(url,timeout=20)\n",
    "    req.html = req.text\n",
    "    soup = BeautifulSoup(req.html,\"html.parser\")\n",
    "    \n",
    "    print('Page',counter,'of search results added.')\n",
    "    \n",
    "    #then get the trial urls and add them to big list of trial urls\n",
    "    for a_tag in soup.find_all('a', href = re.compile('#highlight')):\n",
    "    #    list_urls_page = []\n",
    "        print('https://www.oldbaileyonline.org/'+a_tag['href'])\n",
    "        list_trial_urls.append('https://www.oldbaileyonline.org/'+a_tag['href'])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check: should be 391\n",
    "len(set(list_trial_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "Check if list url is the last on page '39':\n",
    "\n",
    "https://www.oldbaileyonline.org/browse.jsp?id=t18810502-481-offence-1&div=t18810502-481#highlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "## 4. Last step! Now we loop through all the urls of trials, extract the information in question, and save them to two Py lists, and then we'll zip those into a dictionary and pickle it.\n",
    "\n",
    "- We will want to include a 'Try-Except' clause as well in order to let the scrape continue if it hits any snags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#two empty lists\n",
    "text_of_trials = []\n",
    "title_of_trials = []\n",
    "\n",
    "#change to an empty directory to save raw text files\n",
    "os.chdir(\"data_dump\")\n",
    "\n",
    "for u in list_trial_urls:\n",
    "    \n",
    "    # get the soup from each page of URL\n",
    "    url = u\n",
    "    req = requests.get(url,timeout=20)\n",
    "    req.html = req.text\n",
    "    soup = BeautifulSoup(req.html,\"html.parser\")\n",
    "    \n",
    "    #get the title of the trial\n",
    "    try:\n",
    "        title_of_trial = soup.find('div', class_='sessionsPaperTitle').get_text()\n",
    "        title_of_trials.append(title_of_trial)\n",
    "        print('Processed the title of trial:',len(title_of_trials))\n",
    "    except:\n",
    "        print('ERROR on title of trial!')\n",
    "    \n",
    "    try:\n",
    "        #get the trial text\n",
    "        trial_text = [] #blank list, must be outside of 4 loop\n",
    "        #find all links on each page\n",
    "        for p in soup.find('div', class_='sessionsPaper').find_all('p'):\n",
    "            trial_text.append(p.get_text()) #append each paragraph to blank list\n",
    "    \n",
    "        text_of_trials.append(trial_text)\n",
    "        print('Processed the test of trial:',len(text_of_trials))\n",
    "    except:\n",
    "        print(\"ERROR on text of trial!\")\n",
    "    \n",
    "    \n",
    "    #dict_trial = dict(zip(title_of_trials,trial_text))\n",
    "    #pickle.dump(dict_trial,open(\"obo_trials_returningfromtransport.p\",\"wb\"))\n",
    "    \n",
    "    #write out\n",
    "    f = open(title_of_trial,'w')\n",
    "    f.write(str(trial_text))\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    #write out and save local object\n",
    "    #dict_return_transit_trials = dict(zip(title_of_trials,text_of_trials))\n",
    "    #with open('obo_transit_trails.json','w') as outfile: #open local file, write to it\n",
    "    #    outfile = json.dumps(dict_return_transit_trials) #send to outfile this python bit\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data check\n",
    "title_of_trials[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data check\n",
    "text_of_trials[20:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### 5. Save as pickle object\n",
    "\n",
    "- Pickle is the way to save python objects on local disk.\n",
    "- Probably a good idea to pickle while scraping for large scrapes in case there's an error b/c then you'll at least get a chunk of it.\n",
    "- Next week, we'll discuss storing them in more human-friendly ways.\n",
    "- For now, your goal should be to find a place with some interesting data and then scraping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zip each of our lists into a dictionary, in which the key is the title and the value is the text of the trial.\n",
    "obo_return_transit_trials = dict(zip(title_of_trials,text_of_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(obo_return_transit_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# wb stands for \"write binary\", that is, literally store this pickle as binary code\n",
    "pickle.dump(obo_return_transit_trials, open(\"obo_return_transit_trials.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save as JSON\n",
    "\n",
    "* Essentially \"python dictionary object, but the local version!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict_return_transit_trials = dict(zip(title_of_trials,text_of_trials))\n",
    "with open('obo_transit_trails.json','w') as outfile: #open local file, write to it\n",
    "    outfile = json.dumps(dict_return_transit_trials) #send to outfile this python bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Data wrangling in two weeks. For now, we're just focused on pulling the data from offline.\n",
    "\n",
    "\n",
    "### How to srape a website that requires a login:\n",
    "\n",
    "http://kazuar.github.io/scraping-tutorial/\n",
    "\n",
    "\n",
    "- Same as above, but you will have a dictionary containing the login information that will be fed into your request command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "# Homework\n",
    "\n",
    "http://www.presidency.ucsb.edu/sou.php\n",
    "\n",
    "### Scrape all of SOTU speeches. Store them as dictionary objects with each key as \"lastname_firstname_date\" and the value as the text (raw) of each speech. Upload these data to Harvard GDrive (n.b. $\\infty$ space), Dropbox, or Github and post a link to that on Canvas. Also include your IPython notebook.\n",
    "\n",
    "(n.b. In your script, your files should be saving to a place where they are automatically uploaded. This is good practice for getting large datasets and not overloading your storage. The `os.chdir` command changes the work directory to another folder alongside choosing to have that folder auto upload.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
