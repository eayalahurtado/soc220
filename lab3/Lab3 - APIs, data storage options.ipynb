{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "# Soc220: Computational Text Analysis\n",
    "## Lab3: APIs and data storage options\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"tillerson.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "***\n",
    "    2/7/2018\n",
    "    Zach Wehrwein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkpurple>\n",
    "\n",
    "### Solutions\n",
    "\n",
    "* 'class' in python is a default command use to program custom objects and methods, so that's why you use class_ in soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "#change directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd /Users/zwehrwein/Dropbox/Coursework/2018Spring/soc220_spring2018/soc220-labs-draft/lab3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIST OF SPEECH URLS\n",
    "\n",
    "\n",
    "url = \"http://www.presidency.ucsb.edu/sou.php\"\n",
    "req = requests.get(url,timeout=20) #always include a delay!\n",
    "req.status_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "req.html = req.text\n",
    "soup = BeautifulSoup(req.html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nixon.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Need to get right URLS as a result (just in the table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right list of URLS\n",
    "#VER12 gets us the main table and not the extra links\n",
    "speech_url_list = []\n",
    "for span in soup.find_all('td', class_=\"ver12\"):\n",
    "    #print(span)\n",
    "    if len(span.find_all('a',href = re.compile('pid'))) > 0:\n",
    "        link = span.find('a',href = re.compile('pid'))['href']\n",
    "        speech_url_list.append(link)\n",
    "        print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new directory to store data.\n",
    "dir_path = \"sotu_speeches\"\n",
    "if os.path.exists(dir_path) == False:\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "os.chdir('sotu_speeches')\n",
    "   \n",
    "#print working directory\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_speeches = []\n",
    "text_of_speeches = []\n",
    "\n",
    "for url in speech_url_list:\n",
    "    \n",
    "    #10x timeout\n",
    "    req = requests.get(url,timeout=200)\n",
    "    req.html = req.text\n",
    "    soup = BeautifulSoup(req.html,\"html.parser\")\n",
    "    \n",
    "    #date\n",
    "    try:\n",
    "        date = soup.find('span', {'class' : 'docdate'}).get_text()\n",
    "        #dates_of_speeches.append(date)\n",
    "        #print('processed:',len(dates_of_speeches),date)\n",
    "    except:\n",
    "        print('FAIL ON DATE:',len(dates_of_speeches))\n",
    "        \n",
    "    #name\n",
    "    try:\n",
    "        name = soup.title.text.split(':')[0]\n",
    "        last_name = name.split()[-1]\n",
    "        first_name = name.split()[0]\n",
    "        #names_of_prezs.append(name)\n",
    "        #print('processed:',len(names_of_prezs),last_name)\n",
    "    except:\n",
    "        print('FAIL ON NAME:')\n",
    "        \n",
    "    title = last_name+'_'+first_name+'_'+date\n",
    "    titles_speeches.append(title)\n",
    "    print(title)\n",
    "    \n",
    "    #text\n",
    "    speech_pars = []\n",
    "    for p in soup.find('span', class_='displaytext').find_all('p'):\n",
    "        speech_pars.append(p.get_text())\n",
    "        \n",
    "    text_of_speeches.append(str(speech_pars))\n",
    "    print('PROCESSED:',len(text_of_speeches),'th SOTU')\n",
    "    \n",
    "    #write_out too\n",
    "    f = open(title,'w')\n",
    "    f.write(str(speech_pars))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(text_of_speeches)); print(len(titles_speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sotu_dict = dict(zip(titles_speeches,text_of_speeches))\n",
    "pickle.dump(sotu_dict,open(\"sotu_dict.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<font color=darkgreen>\n",
    "\n",
    "## Today:\n",
    "\n",
    "1. How to use APIs\n",
    "2. Twitter API examples: <br>\n",
    "    (a.) #metoo <br>\n",
    "    (b.) #maga <br>\n",
    "    (c.) @realDonaldTrump \n",
    "3. Data storage options (JSON, CSV)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### APIs\n",
    "\n",
    "- A set of protocols for two forms of software to communicate with one another.\n",
    "- Rather than loading a webpage and looking for content, APIs allow us to directly ask another machine for information.\n",
    "- Compared to web scraping, APIs are far more sane\n",
    "- Most major tech firms provide APIs\n",
    "- API  stands for 'application programming interface' and it is essentially a set of tools that allow you to communicate directly with a data server.\n",
    "- So, this means that all the little tricks we discussed last week (finding the right bit of HTML code, using a regex etc) are unnecessary\n",
    "- Moreover, since you are getting consistent streams of data, then you can save these data in a way that is human usable. Recall my warning last week: when scraping, you wish to have the smootest possible request->scrape->save code because if there is any little bump in the road, the whole program can crash.\n",
    "\n",
    "#### Stream vs REST API\n",
    "\n",
    "- Twitter has two types of APIs, one is the stream (a continuous unlimited flow of real time information), while the other is REST -- a repository which is one week old and is rate limited: 180 requests of up to 100 tweets per request every 15 minutes. (1200 tweets a minute)\n",
    "- Twitter is bad about providing historical data, but this is not true of other firms. If you want Twitter data, you'll have to let it run for a week.\n",
    "\n",
    "#### Lots of possible APIs:\n",
    "- Yelp\n",
    "- NYT\n",
    "- Wikipedia\n",
    "- JSTOR\n",
    "- Spotify\n",
    "- Open Movie Database\n",
    "- and many more!\n",
    "\n",
    "#### In general, here's what we'll do:\n",
    "1. Go to the website in question, create a 'developer' account and be given a set of credentials.\n",
    "2a. Go on Github and find the repository of the developer who wrote the API and look for a file like \"sample.py\" that provides an example way to access the API.\n",
    "2b. Following 2a, install the developer  module and feed your credentials.\n",
    "3. Make requests of the API for specific data you want. (Always include a delay so you don't get blocked!)\n",
    "4. If there are restrictions on the API (can only access recent data or you have to pay for more), then we think through work-around.\n",
    "\n",
    "\n",
    "\n",
    "https://developer.twitter.com/en/docs\n",
    "\n",
    "https://apps.twitter.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"twitter1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"twitter2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"twitter3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "### PART1: Collect Tweets from the public 'garden house' using Twitter's own Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twitter import Twitter, OAuth, TwitterStream\n",
    "import json #Twitter outputs data in JSON format, which we will save locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API credentials\n",
    "API_KEY = \"GMCwaYz3FjycjSNamORMsfh6o\"\n",
    "API_SECRET = \"b95joF79aiZfs9b1tSVUliCsJMwZJasBBklHQuSVjcyl3zWgs0\"\n",
    "ACCESS_TOKEN = \"813490235099201536-mCd8VIMWdjpgVZh6D4SCIu9YdSjJ3D4\"\n",
    "ACCESS_SECRET = \"6aBBdZEf6PnGPpLnXFVZT0Jlov3e9kJOQySCbB0ihrmq9\"\n",
    "\n",
    "oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, API_KEY, API_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to Twitter Stream API\n",
    "\n",
    "twitter_stream = TwitterStream(auth=oauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a sample of tweets\n",
    "\n",
    "tweets_sample = twitter_stream.statuses.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sample_tweets = list()\n",
    "tweet_count = 100\n",
    "for tweet in tweets_sample:\n",
    "    \n",
    "    # Count down from 0 to collect 1000 public tweets\n",
    "    tweet_count -= 1\n",
    "    \n",
    "    print(tweet)\n",
    "    list_sample_tweets.append(tweet)\n",
    "    \n",
    "    # Stop this loop once we hit 0.\n",
    "    if tweet_count <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list_sample_tweets[5])\n",
    "\n",
    "# Designed to function like a dictionary object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "- text: the text of the actual tweet;\n",
    "- created_at: the date of creation;\n",
    "- favorite_count: the number of favorites of this tweet;\n",
    "- retweet_count: the number of retweets;\n",
    "- lang: the language of the tweet;\n",
    "- id: the tweet identifier;\n",
    "- place: geo-location information;\n",
    "- user: the profile of the author of the tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_sample_tweets[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sample_tweets[22]['entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "**This isn't that interesting though:**\n",
    "\n",
    "- Single hashtag?\n",
    "- Keep it local?\n",
    "\n",
    "Again, consult the docs:\n",
    "\n",
    "https://twitterapi.readthedocs.io/en/latest/\n",
    "\n",
    "https://developer.twitter.com/en/docs\n",
    "\n",
    "https://github.com/sixohsix/twitter\n",
    "\n",
    "https://github.com/ideoforms/python-twitter-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metoo_tweets_list = list()\n",
    "\n",
    "tweet_count = 10\n",
    "#call the twitter stream, then the statuses method to then filter\n",
    "for tweet in twitter_stream.statuses.filter(track='#metoo',language='en'):\n",
    "    \n",
    "    # Count down from 0 to collect 1000 public tweets\n",
    "    tweet_count -= 1\n",
    "    \n",
    "    \n",
    "    #print text of tweet to see what it says\n",
    "    print(tweet['text'])\n",
    "    \n",
    "    #save data out\n",
    "    metoo_tweets_list.append(tweet)\n",
    "    #json.dumps(metoo_tweets_list)\n",
    "    \n",
    "    json_doc = json.dumps(metoo_tweets_list)\n",
    "    #outputFile.write(json_doc.encode('utf8') + '\\n')\n",
    "    \n",
    "    #open up a json file, call it metoo tweets, write to it\n",
    "    out_json = open('metoo_tweets.json','w')\n",
    "    out_json.write(json_doc)\n",
    "    out_json.close()\n",
    "    \n",
    "    # Stop this loop once we hit 0.\n",
    "    if tweet_count <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metoo_tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maga_tweets_list = list()\n",
    "\n",
    "tweet_count = 10\n",
    "#call the twitter stream, then the statuses method to then filter\n",
    "for tweet in twitter_stream.statuses.filter(track='#maga',language='en'):\n",
    "    \n",
    "    # Count down from 0 to collect 1000 public tweets\n",
    "    tweet_count -= 1\n",
    "    \n",
    "    \n",
    "    #print text of tweet to see what it says\n",
    "    print(tweet['text'])\n",
    "    \n",
    "    #save data out\n",
    "    maga_tweets_list.append(tweet)\n",
    "    #json.dumps(metoo_tweets_list)\n",
    "    \n",
    "    json_doc = json.dumps(maga_tweets_list)\n",
    "    #outputFile.write(json_doc.encode('utf8') + '\\n')\n",
    "    \n",
    "    #open up a json file, call it metoo tweets, write to it\n",
    "    out_json = open('maga_tweets.json','w')\n",
    "    out_json.write(json_doc)\n",
    "    out_json.close()\n",
    "    \n",
    "    # Stop this loop once we hit 0.\n",
    "    if tweet_count <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metoo_tweets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### JSON files\n",
    "\n",
    "- Use the new Firefox browser to open JSON files\n",
    "- out of Python structured as a dictionary (that is, key-value pairs)\n",
    "- json_x.load() will load the json object 'json_x' as a dictionary in Py\n",
    "- Because they are saved locally, this requires some code to tell the computer what to save\n",
    "\n",
    "\n",
    "`JSON_DUMP = json.dumps(ENTITY)` This takes a Python objects and puts into a json object that is now held in memory. <br>\n",
    "`LOCAL_FILE_NAME = open('LOCAL_FILE_NAME'.json, 'w')` 'w' means 'write to file'; this opens a local file. <br>\n",
    "`LOCAL_FILE_NAME.write(JSON_DUMP)` Take that json object you have in memory and write it to the local file. <br>\n",
    "`LOCAL_FILE_NAME.close()` Close and save local file. n.b. the json object is still held in memory at this point, but it's also out of the Python program. <br>\n",
    "\n",
    "Likewise, to open up a local file, do this: <br>\n",
    "`\n",
    "with open(\"LOCAL_FILE_NAME.json\") as JSON_FILE:\n",
    "    back_to_py = json.load(JSON_FILE)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "### PART2: For more detailed searches (beyond just the stream), we'll turn to a wrapper around the Twitter API called tweepy\n",
    "\n",
    "https://github.com/tweepy/tweepy\n",
    "\n",
    "Docs: https://media.readthedocs.org/pdf/tweepy/v3.2.0/tweepy.pdf\n",
    "\n",
    "**n.b tweepy returns a different type of object compared to the Twitter api -- a 'status' object which functions a bit differently -- always important to read the docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#AUTHENTICATE\n",
    "\n",
    "auth = OAuthHandler(API_KEY,API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN,ACCESS_SECRET)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "* `wait_on_rate_limit=True` means that tweepy will exhaust the amount of 'calls' you can make to the API and wait for a refresh. In order to avoid being overwhelmed, most APIs specify a certain number of calls per hour (always check their documentation).\n",
    "\n",
    "Full tweepy docs:\n",
    "\n",
    "http://docs.tweepy.org/en/v3.5.0/getting_started.html\n",
    "\n",
    "\n",
    "https://media.readthedocs.org/pdf/tweepy/latest/tweepy.pdf\n",
    "\n",
    "Best tweepy practices:\n",
    "\n",
    "http://www.nirg.net/using-tweepy.html\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK STATUS IS AN OPTION WITH tweepy\n",
    "print(api.me().name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "## CSV files\n",
    "\n",
    "- 'comma seperated values'\n",
    "- flexible and useful for almost all dataframe like analyses\n",
    "- Not widely used because we're creating a data structure perhaps before we've really thought about it (flat frame)\n",
    "- You have to specify which bits of data that you wish.\n",
    "\n",
    "For 'status' objects created by tweepy, these are the following:\n",
    "\n",
    "* 'author',\n",
    "* 'contributors',\n",
    "* 'coordinates',\n",
    "* 'created_at',\n",
    "* 'destroy',\n",
    "* 'entities',\n",
    "* 'extended_entities',\n",
    "* 'favorite',\n",
    "* 'favorite_count',\n",
    "* 'favorited',\n",
    "* 'geo',\n",
    "* 'id',\n",
    "* 'id_str',\n",
    "* 'in_reply_to_screen_name',\n",
    "* 'in_reply_to_status_id',\n",
    "* 'in_reply_to_status_id_str',\n",
    "* 'in_reply_to_user_id',\n",
    "* 'in_reply_to_user_id_str',\n",
    "* 'is_quote_status',\n",
    "* 'lang',\n",
    "* 'metadata',\n",
    "* 'parse',\n",
    "* 'parse_list',\n",
    "* 'place',\n",
    "* 'possibly_sensitive',\n",
    "* 'retweet',\n",
    "* 'retweet_count',\n",
    "* 'retweeted',\n",
    "* 'retweets',\n",
    "* 'source',\n",
    "* 'source_url',\n",
    "* 'text',\n",
    "* 'truncated',\n",
    "* 'user'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## CODE DOESN'T ITERATE ##\n",
    "\n",
    "import csv\n",
    "\n",
    "# again you're creating a variable which links to a local file\n",
    "file = open('tweets_csv.csv', 'a')\n",
    "\n",
    "# open \n",
    "writer = csv.writer(file)\n",
    "\n",
    "tweet_count = 100\n",
    "\n",
    "while tweet_count > 0:\n",
    "    for tweet in tweepy.Cursor(api.search, q=\"shithole\", language='en').items(10):\n",
    "        tweet_count -= 1\n",
    "    \n",
    "        writer.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "        print(tweet.created_at, tweet.text)\n",
    "    \n",
    "        #the file itself, not the writer is closed.\n",
    "    \n",
    "        if tweet_count <= 0:\n",
    "            break\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "\n",
    "### PART3: getting a handle on the current status of American foreign policy (i.e. downloading @realDonaldTrump's timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trump_tweets = list()\n",
    "all_trump_file = open('all_trump_tweets.csv', 'a')\n",
    "all_trump_writer = csv.writer(all_trump_file)\n",
    "\n",
    "#initial request\n",
    "new_trump_tweets = api.user_timeline(screen_name='realDonaldTrump',count=200)\n",
    "all_trump_tweets.extend(new_trump_tweets)\n",
    "oldest_tweet = all_trump_tweets[-1].id - 1\n",
    "oldest_created_at = all_trump_tweets[-1].created_at\n",
    "\n",
    "while len(new_trump_tweets) > 0:\n",
    "    print('Getting a new round of Trump tweets starting from:', oldest_created_at)\n",
    "    \n",
    "    new_trump_tweets = api.user_timeline(screen_name='realDonaldTrump',count=200,max_id=oldest_tweet)\n",
    "    \n",
    "    all_trump_tweets.extend(new_trump_tweets)\n",
    "    \n",
    "    #write out\n",
    "\n",
    "    for tweet in new_trump_tweets:\n",
    "        out_data = [tweet.created_at,\n",
    "                    tweet.favorite_count,\n",
    "                    tweet.in_reply_to_screen_name,\n",
    "                    tweet.retweet_count,\n",
    "                    tweet.source,\n",
    "                    tweet.text.encode(\"utf-8\")]\n",
    "    \n",
    "        all_trump_writer.writerow(out_data)\n",
    "    \n",
    "    oldest_tweet = all_trump_tweets[-1].id - 1\n",
    "    oldest_created_at = all_trump_tweets[-1].created_at\n",
    "    \n",
    "    time.sleep(60)\n",
    "    \n",
    "    print('\\n',len(all_trump_tweets),'Trump tweets have been collected so far.','\\n')\n",
    "    \n",
    "    \n",
    "all_trump_file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "### Our file is about 35,000 tweets short of getting the whole corpus -- what could we do?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "-- Include delays? <br>\n",
    "-- Call a larger iteration <br>\n",
    "-- Change the dates for 'oldest' tweet? <br>\n",
    "-- Except / Pass error messages. <br>\n",
    "-- Does tweepy have a function to let you see your current call status? Could we build something off that? <br>\n",
    "-- You could just keep running that cell over and over again... but why?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.rate_limit_status()['resources']['users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trump_tweets = list()\n",
    "all_trump_file = open('all_trump_tweets.csv', 'a')\n",
    "all_trump_writer = csv.writer(all_trump_file)\n",
    "\n",
    "#initial request\n",
    "new_trump_tweets = api.user_timeline(screen_name='realDonaldTrump',count=200)\n",
    "all_trump_tweets.extend(new_trump_tweets)\n",
    "oldest_tweet = all_trump_tweets[-1].id - 1\n",
    "oldest_created_at = all_trump_tweets[-1].created_at\n",
    "\n",
    "while len(new_trump_tweets) > 0:\n",
    "    print('Getting a new round of Trump tweets starting from:', oldest_created_at)\n",
    "    \n",
    "    new_trump_tweets = api.user_timeline(screen_name='realDonaldTrump',count=200,max_id=oldest_tweet)\n",
    "    \n",
    "    all_trump_tweets.extend(new_trump_tweets)\n",
    "    \n",
    "    #write out\n",
    "\n",
    "    for tweet in new_trump_tweets:\n",
    "        out_data = [tweet.created_at,\n",
    "                    tweet.favorite_count,\n",
    "                    tweet.in_reply_to_screen_name,\n",
    "                    tweet.retweet_count,\n",
    "                    tweet.source,\n",
    "                    tweet.text.encode(\"utf-8\")]\n",
    "    \n",
    "        all_trump_writer.writerow(out_data)\n",
    "    \n",
    "    oldest_tweet = all_trump_tweets[-1].id - 1\n",
    "    oldest_created_at = all_trump_tweets[-1].created_at\n",
    "    \n",
    "    ### INCLUDE A TIME DELAY###\n",
    "    time.sleep(60)\n",
    "    \n",
    "    print('\\n',len(all_trump_tweets),'Trump tweets have been collected so far.','\\n')\n",
    "    \n",
    "    \n",
    "all_trump_file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkgreen>\n",
    "    \n",
    "### Use the RCE\n",
    "\n",
    "https://rce-docs.hmdc.harvard.edu/book/accessing-rce-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "## Exercise : \n",
    "\n",
    "#### Who does Donald Trump read? Can you write a simple Python scrip to detect when the nuke's gonna fly?\n",
    "\n",
    "In other words, can you recreate Trump's feed in a CSV? Itterate through a list of his friends and call the 5 most recent tweets and save this to a CSV file. Then sort either in a notebook or in the CSV file itself by date to recreate his actual feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trump_friends = []\n",
    "for friend in api.get_user('realDonaldTrump').friends(count=50):\n",
    "    trump_friends.append(friend.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>\n",
    "\n",
    "# TO RECAP:\n",
    "\n",
    "* **APIs are better than scrapping HTML / XML**\n",
    "* **There are data restriction limits however these will be app specific. For instance, Spotify will only give you named Playlists, so you have to find a way to get a large list of playlist names. With Twitter, telling the computer to keep trying after it has hit its limit.**\n",
    "* **Look to Github and docs for examples.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
